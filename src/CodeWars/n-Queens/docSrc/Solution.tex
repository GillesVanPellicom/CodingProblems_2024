\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\lstset{
    basicstyle=\ttfamily, % sets the font for code
    keywordstyle=\color{black}, % keywords in blue
    commentstyle=\color{black}, % comments in green
    stringstyle=\color{black}, % strings in red
    numbers=left, % line numbers on the left
    numberstyle=\tiny\color{black}, % style for line numbers
    stepnumber=1, % show every line
    frame=none, % put a frame around the code
    breaklines=true, % allow line breaking
    language=C++ % specify language (C++, Python, etc.)
}

% paragraph styling
\setcounter{secnumdepth}{4}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

% Preamble
\title{Heuristic Approach for Efficiently Solving Large-Scale n-Queens Problems with a Pre-Specified Mandatory Queen}
\author{Gilles Van pellicom}

\begin{document}


\maketitle

\begin{abstract}
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{1em}
    The n-Queens problem tasks us with placing \(n\) queens on an \(n \times n\) chessboard such that no two queens attack each other.
    This includes ensuring no queens share the same row, column, diagonal or anti-diagonal. Additionally, to make this version of the problem more challanging,
    one queen's position is fixed.
    The mandatory queen placement introduces challenges in certain cases, leading to unsolvable configurations, which are also handeled.
    This explanation explores different approaches to solving the n-Queens problem for varying board sizes up to \(n = 1000\).

    This explanation recognizes the broad scope of the material used and as such will include links to articles explaining specific concepts
    each time they are initially mentioned. These links are embedded and so can only be opened when viewing the digital version of this document.
\end{abstract}

\section{Introduction}
This problem is classically used in computer science to teach beginners the concept of \href{https://en.wikipedia.org/wiki/Backtracking}{algorithmic backtracking}.
Although this approach is quite intuitive, solving the problem this way carries a horribly bad \href{https://en.wikipedia.org/wiki/Time_complexity}{time complexity} of \(O(n!)\).
This is acceptable for small \(n\), but since in this version of the problem \(0 \leq n \leq 1000\), this approach is not feasible.
E.g. even for \(n = 20 \implies 20! = 2.432902 \times 10^{18}\) iterations.

There are quite a few ways of optimizing backtracking for the n-Queens problem, such as \href{https://en.wikipedia.org/wiki/Decision_tree_pruning}{pruning},
\href{https://en.wikipedia.org/wiki/Memoization}{memoization}, and \href{https://www.khoury.northeastern.edu/home/wahl/Publications/ew05b.pdf}{symmetry reduction}.
However, even with these optimizations, the average time complexity will still be a not very spectacular \(O(c^n)\) where \(c \approx [1.5; 2]\) and in the worst case
\(O(n \cdot 2^n)\). Again, this makes large \(n\) infeasible.
Because of these initial observations, I realised I required a different approach.

\section{Attempt one: Simulated Annealing}
\href{https://en.wikipedia.org/wiki/Randomized_algorithm}{Simulated Annealing} (SA) is a \href{https://en.wikipedia.org/wiki/Metaheuristic}{metaheuristic} technique inspired by
\href{https://en.wikipedia.org/wiki/Metallurgy#:~:text=Metallurgy%20is%20a%20domain%20of,which%20are%20known%20as%20alloys.}{metallurgy} where controlled 
\href{https://en.wikipedia.org/wiki/Entropy}{entropy} is utilised to selectively temper out defects and converge to an optimal solution. For a given
\href{https://en.wikipedia.org/wiki/Constraint_satisfaction_problem}{CSP}, such as in this case the n-Queens problem, SA is able to relatively quickly converge,
especially compared to a \href{https://en.wikipedia.org/wiki/Brute-force_attack}{brute-force} approach.

\subsection{Simulated Annealing, Generalised}
Viewed at a high level, SA works as follows: The goal is to bring the system (in this case n-Queens), from an arbitrary initial state,
to a perfect state with the minimum possible energy (conflicts) level.
To utilise simulated annealing two main functions have to be defined:

\begin{itemize}
    \item \textbf{Neighbor Function}: This function takes the current state of the board, makes a small random change, and returns the resulting state.
    \item \textbf{Energy Function}: This function computes the energy of a given state, which in this context corresponds to the number of conflicts.
\end{itemize}

The algorithm starts with a specific temperature, which cools by a specified factor each iteration.
The temperature controls the likelihood of a counter-productive neighbor being accepted.
This is done so that the solution space is sufficiently traversed in the hopes of arriving at an optimal solution.

\subsubsection{Notation}

\begin{itemize}
    \item \(\boldsymbol{T}\): current temperature.
    \item \(\boldsymbol{T_{\mathbf{min}}}\): minimum temperature.
    \item \(\boldsymbol{\alpha}\): cooling factor.
    \item \(\boldsymbol{E_{\mathbf{current}}}\): current board energy.
    \item \(\boldsymbol{E_{\mathbf{neighbor}}}\): neighbor board energy.
    \item \(\boldsymbol{\Delta E} = E_{\text{neighbor}} - E_{\text{current}}\)
\end{itemize}

\subsection{Main algorithm}

Each iteration, \(T\) is multiplied by \(\alpha\), \(T \coloneqq T \cdot \alpha\),
to cool the temperature and lower the chances of accepting a worse neighbor as the algorithm gets closer to a solution. More specifically:

\[
    \text{If } \left(\Delta E < 0 \lor \exp\left(\frac{-\Delta E}{T}\right) > x \text{ such that } x \in [0, 1] \right)
\]
\[
    \text{Then accept the neighbor.}
\]

Where \( \exp(a) = e^a \), \( e \) is \href{https://en.wikipedia.org/wiki/E_(mathematical_constant)}{Euler's number},
and \( x \) is randomized each iteration within given constraints.

To elaborate on the acceptance criteria, there are basically three paths leading to two outcomes:

\begin{enumerate}
    \item \(\boldsymbol{\Delta E < 0}\). If the difference in energy is negative, the neighbor is better. Accept the neighbor.
    \item \(\boldsymbol{\exp\left(\frac{-\Delta E}{T}\right) > x \textbf{ such that } x \in [0, 1]}\)
          If the first criterion is not met, calculate the chance this neighbor will be accepted based on the difference in energy,
          the current temperature, and if that chance is greater than a randomly generated number \(x\) between \(0\) and \(1\) \((x \in \mathbb{R})\), accept the neighbor.
    \item \textbf{If both criteria fail}, discard the neighbor.
\end{enumerate}

\subsection{Critical Issues}
Unfortunately there are two main glaring issues with using simulated annealing for n-Queens with large \(n\).

\subsubsection{Critical Issue 1: Stagnation}
SA has the tendancy to stagnate (get stuck in local minima). Meaning it gets stuck on almost-but-not-really optimal solutions,
usually within a \(2-3\%\) range of an optimal \(E = 0\) solution.
This is acceptable for some problems such as the \href{https://en.wikipedia.org/wiki/Travelling_salesman_problem}{travellign salesman problem},
where complete accuracy isn't a requirement for the solution to be practical, but that isn't the case for n-Queens.

This could be solved by implementing a stagnation detection and resolving algorithm.
The problem with doing this is that you have to define when something counts as stagnation,
what actions to take when stagnation has been detected, how much action to take and when to take it.
Meaning quite a few new variables which need adjusting would have to be introduced on top of the very sensitive annealing variables.
These variables also can't be constant and have to be in function of \(n\).

Profiling and solving this very delicate balancing act is a large time investment with no real guarantee of success in the end,
especially when combined with the next issue.

\subsubsection{Critical Issue 2: Convergence}
As previously stated, SA is efficient at getting close to an optimal solution.
Actually converging from a board with e.g. \(E=8\) to \(E=0\) is a different story.
This isn't very efficient since SA is basically a random number generator with no real idea of what it's working toward.
The amount of time required to fully converge gets very steep as \(n \to 1000\).

\subsection{Conclusion}
Because of the two major issues stated above, SA was deemed to be a poor fit for this particular problem.

However, the concept of dynamically adjusting agressiveness of action based on how close
to a solution the algorithm currently is was implemented in my final solution.

\section{Attempt one: Simulated Annealing}
\href{https://en.wikipedia.org/wiki/Min-conflicts_algorithm}{Min-conflicts}
is a heuristics-based search algorithm which shines in quickly converging from a semi-high energy state to a perfect state (\(E = 0\)).

The algorithm creates a list of all queens with the very highest \(E\) values (most conflicts),
picks out one of the queens from that list at random and places it in a new location
which has been calculated to have lowest possible \(E\) value out of all moves which can be made with that queen.
If more than one lowest energy location exists, pick one at random as to more effectively explore the entire solution space.

This approach is much more efficient and simpler for this particular problem compared to SA since there aren't any tempermental variables to tangle with.

\subsection{Critical Issues}
Even though this approach should be much better suited to this problem, there are still issues.
\subsubsection{Critical Issue 1: Stagnation}
As with SA, stagnation is a big issue. Especially with small \(n<10\), where this algorithm has a tendancy to take tens of minutes over something which could conceivably be calculated in a few milliseconds.
The algorithm stagnates about 50\% of the time for small \(n\).
Min-conflicts doesn't have very sensitive variables as with SA,
so I felt much more comfortable putting in the time of running test-suites to find the optimal formulae to calculate the variables in order to fine-tune a sophisticated stagnation D/R (detection and resolution) system.

\paragraph{Notation}
\begin{itemize}

    \item \(iterationCount\): amount of times the algorithm has looped.
    \item \(stagnationCheckInterval\): amount of iterations between checks, calculated as \(10n\).
          \((stagnationCheckInterval \in \mathbb{Z}^+)\). In other words, every \(iterationCount \equiv 0 \pmod{10n}\), stagnation D/R runs.
\end{itemize}

\paragraph{Solution}
To resolve stagnation, the computer first has to be able to detect stagnation, so stagnation has to be defined.
I have defined it as "if three samples of the board's total energy, taken at specified points within $stagnationCheckInterval$,
and the current total energy are all equal, the board has stagnated".

The algorithm periodically takes energy samples of the entire board.
Here are the criteria for when to take each sample:
\begin{itemize}

    \item \(\mathbf{S_1}\): at \(stagnationCheckInterval\)
    \item \(\mathbf{S_2}\): at \( \left\lfloor \frac{stagnationCheckInterval}{1.5} \right\rfloor\)
    \item \(\mathbf{S_3}\): at \( \left\lfloor \frac{stagnationCheckInterval}{3.0} \right\rfloor\)
\end{itemize}

When dividing as with $S_2$ and $S_3$, the $floor()$ function is utilised so that modulo checks with $iterationCount$, which is $\mathbb{Z}^+$,
don't consistently fail.

By using tri-sampling instead of only one sample, the chance of false positives is lowered significantly.
This and the fact that all samples have to match up perfectly without margin for error makes it so there are significantly less false positives.
I've found that when the algorithm is actually stuck energy fluctuations are bascially non-existent, so I'm able to get away with these strict requirements.

Once detected, a stagnation is resolved by \href{https://en.wikipedia.org/wiki/Perturbation_theory}{perturbation}.
In essence this means randomly making a pre-specified amount of moves not taking into account if the move is an overall benifit or detriment to the solution,
in an attempt to escape local minima.

The perturbation amount must be defined in function of $n$, since using a constant wouldn't scale.
This could be defined as something static such as $\frac{n}{4}$,
but this way you increase the chances of wasting computation time the closer a stagnation occurs towards the end
since you now have higher chances of wasting a carefully calculated correct queen placement each perturbation.

My method of resolving this potential waste of resources is by calculating the perturbation amount dynamically using a modified version of SA.
Instead of using temperature,
this version is based on the ratio of the total board energy compared to the energy at the start of the entire problem where the closer the current board energy is to the start,
the more agressive the algorithm is. By being more gentle toward the end, there is much less chance of loosing a good board setup and having to start over.

\[
    p = \left\lceil p_{min}+\frac{E_{current}}{E_{begin}} \left(p_{max} - p_{min}\right)\right\rceil
\]

Where:
\begin{itemize}
    \item $\mathbf{p}$: calculated amount of perturbations
    \item $\mathbf{p_{min}}$: minimum amount of perturbations
    \item $\mathbf{p_{max}}$: maximum amount of perturbations
    \item $\mathbf{E_{begin}}$: board energy at initialization
    \item $\mathbf{E_{current}}$: current board energy
\end{itemize}

In practice the following numbers were used:
\[
    p = \left\lceil 1+\frac{E_{current}}{E_{begin}} \left(\frac{n}{10} - 1\right)\right\rceil
\]

This way, even when $E\to0$, a positive stagnation check will result in a positive stagnation check will result in at least one perturbation, and at most $\frac{n}{10}$ perturbations.
Making too many alterations, e.g. $\frac{n}{4}$, is detrimental to the carefully computed board min-conflicts has constructed. Local minima in later stages can be escaped by smaller, non-overzealous perturbations.

Since this algorithm makes such extensive use of $E$-values, this seems like a good time to explain how these are calculated.


\subsubsection{Critical Issue 2: Initial Convergence}
The second large problem is initial convergence. In other words going from the initial energy of the arbitrary starting board (e.g. $E=1800$)
to a lower energy which min-conflicts can comfortably work with (e.g. $E=100$).

This isn't a big issue with smaller $n$ but as $n \to 1000$ this starts getting very time-consuming.
Enough so that this algorithm is incapable of solving texttt{nQueens(1000)} withing the given time constraint of 160000 ms, making this a critical issue.

Although there are many possible solutions for this issue, after a large amount trial-and-error,
I settled on altering the function which generated the starting layout arbitrarily to make use of a heuristic so that it creates
a starting-board with energy-levels much closer to the eventual solution.

The heuristic I selected to impelement is the \href{https://en.wikipedia.org/wiki/Greedy_algorithm}{greedy algorithm} heuristic.
Basically by making locally-optimal choices, choiches which only serve to minimize energy at that moment and not globally,
the algorithm can place the queens such that the starting energy is much more managable.

Implementation initially caused a spike in stagnations and average execution times since this approach eliminated any randomness
but concequently also caused the solution space to be less effectively explored. The simple solution for this minor issue was to introduce a small amount of randomness
into the function.

Since this function didn't seem to be named I've dubbed it "gan-init" (Greedy-Random Initialization).

Gan-init exponentially improved average execution times causing me to mark this issue as solved.

\subsection{On the computation of E}
The $E$-value or energy of a queen is the amount of conflicts that specific queen currently has with other queens, in other words,
how many other queens she can attack from her current position. How a queen can attack is defined in
\href{https://en.wikipedia.org/wiki/Queen_(chess)\#Placement\_and\_movement}{the rules of chess}.

To compute E, the following methods were utilised:

\subsubsection{Line Indexing}
In a matrix (such as a chessboard), each diagonal, anti-diagonal, horizontal, and vertical line can be given an index, which could be seen as a name of sorts.

E.g., following is a table constructed using the formula $i = x-y$ to illustrate the concept of indexed diagonals:

\[
    \begin{array}{c|cccccccc}
                   & \mathbf{0} & \mathbf{1} & \mathbf{2} & \mathbf{3} & \mathbf{4} & \mathbf{5} & \mathbf{n} & x \rightarrow \\
        \hline
        \mathbf{0} & 0          & 1          & 2          & 3          & 4          & 5          & \cdots                     \\
        \mathbf{1} & -1         & 0          & 1          & 2          & 3          & 4          & \cdots                     \\
        \mathbf{2} & -2         & -1         & 0          & 1          & 2          & 3          & \cdots                     \\
        \mathbf{3} & -3         & -2         & -1         & 0          & 1          & 2          & \cdots                     \\
        \mathbf{4} & -4         & -3         & -2         & -1         & 0          & 1          & \cdots                     \\
        \mathbf{5} & -5         & -4         & -3         & -2         & -1         & 0          & \cdots                     \\
        \mathbf{n} & \vdots     & \vdots     & \vdots     & \vdots     & \vdots     & \vdots     & \ddots                     \\
        y \downarrow                                                                                                          \\
    \end{array}
\]


As seen above, diagonals can be indexed or "named" e.g., "$-1$" or "$3$".
By using this concept, you can compute if any diagonals or horizontals are being shared relatively cheaply.

\subsubsection{Datatype}
Due to the nature of this problem, a valid solution means that there can be only one queen per row, so there is no need to represent the chessboard with an actual 2D-array.
I've defined this datatype to represent the board:

\begin{lstlisting}
using Board = std::vector<int>;
\end{lstlisting}
This c++ code defines a list of integers as a new datatype named "\texttt{Board}".

This way, the index of the array represents the row, and the value represents the column.
Limiting a queen's movements to one column also significantly improves the chances of a change being positive, and this cuts the number of required comparisons per queen per iteration by a factor of $n$.

\subsubsection{Calculation}
Using these insights, definitions and formulas can be constructed.
A conflict is defined as:
\[
    conflict = (b[x] = b[y]) \lor (|b[x] - b[y]| = |x - y|)
\]
Where $b = \texttt{Board}$ (zero-indexed) and $[ ]$, indexing operator.

Using this definition, a formula to compute the board's total energy can be constructed:
\[
    E = \sum_{i=0}^{n-1} \sum_{j=i+1}^{n-1} \left[ ((b[x] = b[y]) \lor (|b[x] - b[y]| = |x - y|)) \right]
\]

This formula iterates over all pairs $(x, y)$ where $x, y \in \mathbb{Z}^+$, $x \leq n \land y \leq n$.
If this pair satisfies one of the constraints, $E$ is incremented by one.
Per pair, one and only one constraint can be satisfied at a time, so there is no loss in accuracy by compressing all constraints into one large constraint.

\subsection{Final Phase}
Since the algorithm was now capable of efficiently calculating n-Queens with large $n$ in a timely manner,
there were a few more constraints to which have to be added in order to comply with the problem definition.

\subsubsection{Base Cases}
The only thing left to take into account are the base cases.
Cases in which the algorithm is not applicable or cannot be used in order to determine the result based on the given arguments.

\begin{itemize}
    \item $\boldsymbol{n = 1}$: Trivial solution. Only one location so only one queen possible. Only one queen so no possible attacks. Handle by always returning a queen in cell $(0, 0)$.
    \item $\boldsymbol{\{ n \in \mathbbm{Z}^{+} \mid n < 4 \land n \neq 1\}}$: No solutions possible. Handle by always returning failiure.
\end{itemize}

\subsubsection{Mandatory Queen}
The problem definition specifies an argument \texttt{std::pair<int mandatoryRow, int mandatoryColumn>}.
This c++ code can be translated as a pair $(x, y)$ which specifies the location for a queen which must be present in the final solution.
This is mostly there to increase the difficulty of the problem, as it introduces new challenges and considerations.

To implement this, one can simply modify \texttt{gan-init()} to place the mandatory queen as the first queen and then construct the initial board around that queen.
For min-conflicts, you simply skip the row where the mandatory queen is located so that it doesn't get included in any lists and, by extension, doesn't get altered.

The problem this causes is that now, especially for smaller boards where \(4 \leq n \leq 7\),
you can have queen placements that block all possible solutions. These so-called "queen-blockades" cannot be resolved.
In other words, they must be detected and the appropriate response for "invalid" has to be returned.

For example, there are only two valid configurations for \(n = 4\):

\begin{center}
    \begin{minipage}{0.45\textwidth}
        \centering
        \[
            \textbf{Solution 1:}
        \]
        \begin{tabular}{|c|c|c|c|}
            \hline
                                      &                           & $\mathbf{Q}_{\mathbf{3}}$ &                           \\
            \hline
            $\mathbf{Q}_{\mathbf{1}}$ &                           &                           &                           \\
            \hline
                                      &                           &                           & $\mathbf{Q}_{\mathbf{4}}$ \\
            \hline
                                      & $\mathbf{Q}_{\mathbf{2}}$ &                           &                           \\
            \hline
        \end{tabular}

        \[
            [(0, 1), (1, 3), (2, 0), (3, 2)]
        \]
    \end{minipage}
    \hspace{0.05\textwidth}
    \begin{minipage}{0.45\textwidth}
        \centering
        \[
            \textbf{Solution 2:}
        \]
        \begin{tabular}{|c|c|c|c|}
            \hline
                                      & $\mathbf{Q}_{\mathbf{2}}$ &                           &                           \\
            \hline
                                      &                           &                           & $\mathbf{Q}_{\mathbf{4}}$ \\
            \hline
            $\mathbf{Q}_{\mathbf{1}}$ &                           &                           &                           \\
            \hline
                                      &                           & $\mathbf{Q}_{\mathbf{3}}$ &                           \\
            \hline
        \end{tabular}

        \[
            [(0, 2), (1, 0), (2, 3), (3, 1)]
        \]
    \end{minipage}
\end{center}


If a mandatory queen position does not match any of these positions, that queen will block any possible solution and the appropriate response has to be returned.

The best way to detect if any given input is valid is by examining the \(iterationCount\).
Due to earlier test-suite results, the average number of iterations for any given \(n\) is known.
Thus, it is only a matter of introducing a stopping mechanism whenever the algorithm exceeds the average number of iterations
plus some extra margin to minimize false negatives in case of very bad luck.

\subsection{Conclusion}
Altough pure min-conflicts isn't able to handle this problem, by implementing the modifications and solutions outlined above,
this method is very efficient at computing the n-Queens problem

\section{Personal Notes}
I imposed the constraint on myself to solve this problem only with general methods for CSP's,
without looking at whitepapers outlining custom n-Queens solving algorithms. This to force
myself to learn more universally applicable concepts.

This problem wasn't easy in the slightest but I managed to get it in the end and I had some fun along the way.

\end{document}